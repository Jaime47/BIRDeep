{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import fnmatch\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from PIL import Image\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "import os\n",
    "import cv2\n",
    "import datetime\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from tensorflow.keras.optimizers import legacy as legacy_optimizers\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras import regularizers\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = np.load(\"class_names.npy\", allow_pickle=True).item()\n",
    "\n",
    "train_df = pd.read_csv(\"train_images.csv\")\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "train_images = os.path.join(current_dir, 'train_images')\n",
    "\n",
    "# Load and preprocess images\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for index, row in train_df.iterrows():\n",
    "    img_filename = row['image_path'][1:]  # Remove leading \"/\"\n",
    "    img_path = os.path.join(train_images, img_filename)\n",
    "    label = row['label'] - 1\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    \n",
    "    # Ensure that the image has 3 channels\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (299, 299))\n",
    "    img = img / 255.0\n",
    "\n",
    "    X.append(img)\n",
    "    y.append(label)\n",
    "\n",
    "X_train = np.array(X)\n",
    "y_train = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HYPERPARAMETER TUNING\n",
    "\n",
    "def create_model(optimizer=legacy_optimizers.Adam(lr=0.001), neurons=256, reg_strength=0.01, learning_rate=0.001):\n",
    " \n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(neurons, activation='relu', kernel_regularizer=regularizers.l2(reg_strength)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(len(class_names), activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Define hyperparameters to search\n",
    "param_grid = {\n",
    "    'neurons': [512], # 128, 256\n",
    "    'reg_strength': [0.01, 0.001],\n",
    "    'learning_rate': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "for neurons in param_grid['neurons']:\n",
    "    for reg_strength in param_grid['reg_strength']:\n",
    "        for learning_rate in param_grid['learning_rate']:\n",
    "           \n",
    "            model = create_model(neurons=neurons, reg_strength=reg_strength, learning_rate=learning_rate)  # Create and compile the model with current hyperparameters\n",
    "\n",
    "            # Train the model\n",
    "            history = model.fit(X_train, y_train, epochs=1, batch_size=32, verbose=1, validation_data=(X_val, y_val))\n",
    "            val_accuracy = history.history['val_accuracy'][0]\n",
    "\n",
    "            if val_accuracy > best_score:\n",
    "                best_score = val_accuracy\n",
    "                best_params = {'neurons': neurons, 'reg_strength': reg_strength, 'learning_rate': learning_rate}\n",
    "                print(\"Validation accuracy: {:.4f} for {}\".format(val_accuracy, best_params))\n",
    "\n",
    "print(\"Best validation accuracy: {:.4f} using {}\".format(best_score, best_params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [

    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(299, 299, 3))\n",
    "\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu',kernel_regularizer=regularizers.l2(0.001)))\n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dense(len(class_names), activation='softmax'))  # best hyperparameters found\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=2, validation_data=(X_val, y_val), callbacks=[early_stopping]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model20.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model20.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    
    "test_df = pd.read_csv(\"test_images_path.csv\")\n",
    "current_dir = os.getcwd()\n",
   
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_images = os.path.join(current_dir, 'test_images')  # Use current_dir instead of script_dir\n",
    "\n",
   
    "X_test = []\n",
    "\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    img_filename = row['image_path'][1:]  # Remove leading \"/\"\n",
    "    img_path = os.path.join(test_images, img_filename)\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    \n",
    "    
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    img = cv2.resize(img, (299, 299))\n",
    "    img = img / 255.0\n",
    "\n",
    "    X_test.append(img)\n",
    "\n",
    "# Convert list to NumPy array\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Update the 'label' column in the test dataframe with the predicted labels\n",
    "test_df['label'] = predicted_labels\n",
    "\n",
    "selected_columns = ['id', 'label']\n",
    "test_df_selected = test_df[selected_columns]\n",
    "\n",
    "# Generate a timestamp for the filename\n",
    "current_datetime = datetime.datetime.now()\n",
    "timestamp = current_datetime.strftime(\"%Y%m%d_%H%M\")\n",
    "file_name = f\"submission_{timestamp}.csv\"\n",
    "\n",
    "submissions_folder = os.path.join(current_dir, 'submissions')  # Use current_dir instead of script_dir\n",
    "\n",
    "# Specify the full file path\n",
    "file_path = os.path.join(submissions_folder, file_name)\n",
    "\n",
    "# Save the final submission file\n",
    "test_df_selected.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    
    "\n",
    "from collections import Counter\n",
    "\n",
    "class_counts = Counter(y_train)\n",
    "print(\"Class distribution before augmentation:\", class_counts)\n",
    "\n",
    "max_images_per_class = 25\n",
    "\n",
    "X_augmented = []\n",
    "y_augmented = []\n",
    "\n",
    "for class_label, count in class_counts.items():\n",
    "    class_indices = np.where(y_train == class_label)[0]\n",
    "    num_samples = len(class_indices)\n",
    "\n",
    "    if num_samples <= max_images_per_class:\n",
    "        # If the class has less than or equal to 15 samples, no augmentation needed\n",
    "        indices_to_augment = class_indices\n",
    "    else:\n",
    "        # If the class has more than 15 images, select a random subset of 15\n",
    "        indices_to_augment = np.random.choice(class_indices, size=max_images_per_class, replace=False)\n",
    "        \n",
    "    # Append original or selected augmented samples to the lists\n",
    "    for idx in indices_to_augment:\n",
    "        img = X_train[idx]\n",
    "        \n",
    "        augmented_img = np.fliplr(img)  # Horizontal flip\n",
    "\n",
    "        X_augmented.append(augmented_img)\n",
    "        y_augmented.append(class_label)\n",
    "    \n",
    "    # If the class had fewer than 15 samples, augment it until it reaches 15\n",
    "    if num_samples < max_images_per_class:\n",
    "        samples_to_augment = max_images_per_class - num_samples\n",
    "        \n",
    "        for idx in np.random.choice(class_indices, size=samples_to_augment, replace=True):\n",
    "            img = X_train[idx]\n",
    "            \n",
    "            # Apply augmentation techniques as above\n",
    "            augmented_img = np.fliplr(img)  # Horizontal flip\n",
    "\n",
    "            X_augmented.append(augmented_img)\n",
    "            y_augmented.append(class_label)\n",
    "\n",
    "# Convert lists to NumPy arrays for augmented data\n",
    "X_augmented = np.array(X_augmented)\n",
    "y_augmented = np.array(y_augmented)\n",
    "\n",
    "# Check the total number of augmented samples\n",
    "print(\"Total augmented samples:\", len(X_augmented))\n",
    "\n",
    "# Shuffle the augmented data\n",
    "shuffle_indices = np.random.permutation(len(X_augmented))\n",
    "X_augmented = X_augmented[shuffle_indices][:max_images_per_class * len(class_counts)]\n",
    "y_augmented = y_augmented[shuffle_indices][:max_images_per_class * len(class_counts)]\n",
    "\n",
    "# Create arrays to hold the final balanced dataset\n",
    "X_balanced = X_augmented[:max_images_per_class * len(class_counts)]\n",
    "y_balanced = y_augmented[:max_images_per_class * len(class_counts)]\n",
    "\n",
    "balanced_class_counts = Counter(y_balanced)\n",
    "print(\"Class distribution after augmentation:\", balanced_class_counts)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
